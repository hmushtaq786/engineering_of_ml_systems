{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An introduction to Optuna\n",
    "This tutorial guides you through the basic use of Optuna, a Python library for automatic hyperparameter optimization in machine learning. The documentation of Optuna can be found [here](https://optuna.readthedocs.io/en/stable/index.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna concepts\n",
    "Three fundamental concepts of Optuna are introduced to help you understand how Optuna works.\n",
    "\n",
    "**1. Objective function**: An objective function is a user-defined function that takes an Optuna `trial` as its argument. For now, it is enough to understand that a trial is a single round of a hyperparameter optimization process. More detailed explanation will be provided later. An objective function specifies the configuration of hyperparameters to be tuned and returns an evaluation score. An example of an objective function is shown below:\n",
    "```python\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"criterion\": trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"]),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 10),\n",
    "        \"min_samples_split\": trial.suggest_float(\"min_samples_split\", 0.1, 1)\n",
    "    }\n",
    "    \n",
    "    model = RandomForestClassifier(**params)\n",
    "    model.fit(train_x, train_y)\n",
    "\n",
    "    predictions = model.predict(test_x)\n",
    "    accuracy = accuracy_score(y_true=test_y, y_pred=predictions)\n",
    "    return accuracy\n",
    "``` \n",
    "As you can see from the example, an objective function typically consists of three parts:\n",
    "1. The search space of each hyperparameter to be tuned. For example, for the `n_estimators` hyperparameter, it is an integer parameter and its value is between 100 and 1000. Similarly, for the `criterion` hyperparameter, it is a categorical parameter and its value is chosen from a fixed list. \n",
    "2. Model training. In the example, an Sklearn RandomForest model is trained.\n",
    "3. Model evaluation (or hyperparameter evaluation). In the example, the model is evaluated using the accuracy score (i.e., the fraction of correct predictions) and the score is finally returned. \n",
    "\n",
    "**2. Trial**: Previously, a trial has been simply mentioned as a single round of a hyperparameter optimization process. More specifically, it is a single execution of an objective function. In each trial, for each hyperparameter defined in the objective function, Optuna selects a value from the search range, evaluates the selected hyperparameter combination and return the evaluation metric.\n",
    "\n",
    "**3. Study**: A study, consisting of multiple trials, refers to an entire hyperparameter optimization process. A study determines the next combination of hyperparameters to be evaluated and keeps track of the trials' results (i.e., the selected hyperparameter combinations and the resulted evaluation metrics). \n",
    "```python\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=42),\n",
    "    storage=\"sqlite:///optuna_tutorial.sqlite3\"\n",
    ")\n",
    "study.optimize(objective, n_trials=10)\n",
    "```\n",
    "In the example, a study is created. As the `direction` parameter indicates, the target of the study is to maximize the objective value (i.e., the accuracy) returned by the objective function. The `sampler` parameter receives an Optuna `Sampler` object. A `Sampler` decides how to explore and exploit the search space for each hyperparameter based on the results of previous trials to efficiently find better hyperparameter combinations. In the example, the study uses the `TPESampler` that uses TPE (Tree-structured Parzen Estimator) algorithm to determine how to choose hyperparameter values for a new trial. \n",
    "\n",
    "When the `study.optimize()` is called, an optimization process is started. The objective function and the number of trials are configured inside the `study.optimize()` function. Optionally, the storage, where the study results are persisted for later analysis, is also specified inside the function. Optuna uses a relational database to store study results. In the example, the database is a simple SQLite file.\n",
    "\n",
    "<details>\n",
    "    <summary>What is SQLite? </summary>\n",
    "    [SQLite](https://www.sqlite.org/index.html) is a lightweight relational database management system. It allows you to create a database as a file, so you can manage and interact with a database without the need for a separate database server. \n",
    "</details>\n",
    "\n",
    "*More reading material: We are not going to the details of the sampling algorithms used by Optuna. For your interest, more details of the sampling algorithms can be found [here](https://optuna.readthedocs.io/en/stable/reference/samplers/index.html).*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna Examples\n",
    "You will see three examples that demonstrate the use of Optuna. The [breast cancer dataset](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic) is used in all of the three examples. \n",
    "\n",
    "In these examples, you will train ML models to classify whether a breast tumour is malignant or benign based on certain features of the tumour. Then you will use Optuna to find the optimal hyperparameter combination for the models. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "X, y = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "print(f\"Feature shape: {X.shape}\")\n",
    "display(X.head())\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of malignant (0) benign (1) tumours\n",
    "y.value_counts() / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a fixed random seed for reproducibility\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.25, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: The basic use of Optuna\n",
    "In this example, you will train an Sklearn's RandomForest model for the breast tumour classification use case. [Area Under the Receiver Operating Characteristic Curve (ROC AUC)](https://scikit-learn.org/stable/modules/model_evaluation.html#receiver-operating-characteristic-roc) is used for model evaluation. \n",
    "\n",
    "The hyperparameters to be configured are listed below:\n",
    "|Hyperparameter| Explanation|type|\n",
    "|--------------| -----------|----|\n",
    "|n_estimators  | The number of trees. | Integer|\n",
    "|criterion     | The function to measure the quality of a split.|String|\n",
    "|max_depth     | The maximum depth of the tree.|Integer|\n",
    "|min_samples_split|The minimum fraction of samples required to further split an internal node|Float|\n",
    "|random_state|Control the randomness in model training. It's fixed in the example for reproducibility|Integer|\n",
    "\n",
    "\n",
    "*If you are interested, more explanation of the hyperparameters of the sklearn random forest classification model can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"criterion\": trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"]),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 10),\n",
    "        \"min_samples_split\": trial.suggest_float(\"min_samples_split\", 0.1, 1),\n",
    "        \"random_state\": RANDOM_SEED\n",
    "    }\n",
    "    \n",
    "    model = RandomForestClassifier(**params)\n",
    "    model.fit(train_x, train_y)\n",
    "\n",
    "    # Predict class probabilities\n",
    "    predictions = model.predict_proba(test_x)\n",
    "\n",
    "    # In binary classification, the probability of the class with the “greater label” should be provided. \n",
    "    # The “greater label” corresponds to classifier.classes_[1] and thus classifier.predict_proba(X)[:, 1].\n",
    "    # See https://scikit-learn.org/stable/modules/model_evaluation.html#binary-case\n",
    "    score = roc_auc_score(test_y, predictions[:, 1])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run a study\n",
    "study_name = \"rf-breast-tumour\"\n",
    "storage = \"sqlite:///optuna_tutorial.sqlite3\"\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED),\n",
    "    study_name=study_name,\n",
    "    storage=storage,\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a SQLite database file \"optuna_tutorial.sqlite3\" appear under the same directory with this notebook. The results of study \"rf-breast-tumour\" is saved in the \"optuna_tutorial.sqlite3\" file. \n",
    "\n",
    "After the study is completed, you can load the study from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_study = optuna.load_study(study_name=study_name, storage=storage)\n",
    "\n",
    "# Check the best trial\n",
    "print(f\"The best roc_auc_score: {loaded_study.best_value}\")\n",
    "print(f\"The best hyperparameter combination: {loaded_study.best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna also provides functionalities for visualizing a study. For example, you can plot a figure to see how much a hyperparameter matters to the objective value (i.e., the roc_auc_score in this case).\n",
    "\n",
    "*If you are interested, more documentation of the Optuna's visualization functionalities can be found [here](https://optuna.readthedocs.io/en/stable/reference/visualization/index.html).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "\n",
    "# Configure Jupyter Notebook to render plotly figures drawn by Optuna\n",
    "pio.renderers.default = \"notebook\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different algorithms can be used to evaluate hyperparameter importance. The default one used by Optuna\n",
    "is the [*fANOVA importance evaluator*](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.importance.FanovaImportanceEvaluator.html#optuna.importance.FanovaImportanceEvaluator). The fixed `RANDOM_SEED` is assigned to the evaluator's seed for reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = optuna.visualization.plot_param_importances(loaded_study, evaluator=optuna.importance.FanovaImportanceEvaluator(seed=RANDOM_SEED))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that once a study is already stored in a database, an error will be raised if you try to record another study with the same name. There are two workarounds for this issue:\n",
    "1. Explicitly setting the argument `load_if_exit=True` when calling `optuna.create_study()`. This will cause Optuna to append the new results to the existing ones for that study.\n",
    "2. Delete the existing study using the code provided in the next code cell.\n",
    "\n",
    "In this week's tutorial and assignments, we choose the second approach to ensure reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the code below if you want to delete the study in the database\n",
    "\n",
    "# study_name = \"rf-breast-tumour\"\n",
    "# storage = \"sqlite:///optuna_tutorial.sqlite3\"\n",
    "# optuna.delete_study(study_name=study_name, storage=storage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Tuning hyperparameters for multiple models -- Conditional search spaces with Optuna\n",
    "In some use cases, you may want to compare the performance of different model libraries or model structures. In these cases, Optuna can be used to optimize the hyperparameters for multiple models in a single study.\n",
    "\n",
    "In this example, you will see how to tune hyperparameters for multiple models using Optuna, where the hyperparameters to be tuned, their search spaces and the model fitting are customized based on different model types. Specifically, we'll tune a logistic regression model and a random forest model and see which one performs better in the breast cancer use case. The hyperparameters to be configured for the random forest model are the same as Example 1. The hyperparameters of the logistic regression model are listed below.\n",
    "|Hyperparameter| Explanation|type|\n",
    "|--------------| -----------|----|\n",
    "|penalty  | Type of regularization to apply to the model. | String|\n",
    "|C     | Control the strength of the regularization. Smaller value leads to stronger regularization|Float|\n",
    "|solver     | The algorithm used to minimize the cost function. It's fixed in the example|String|\n",
    "|max_iter|Maximum number of iterations taken for the solver to converge.|Integer|\n",
    "|random_state|Control the randomness in model training. It's fixed in the example for reproducibility|Integer|\n",
    "\n",
    "*If you're interested, more details of the hyperparameters for an sklearn logistic regression model can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    # We treat the model type as a hyperparameter. In this way, Optuna will perform more trials experimenting with the more promising model type.\n",
    "    classifier_name = trial.suggest_categorical(\"classifier\", [\"logit\", \"rf\"])\n",
    "    \n",
    "    # When the model is a logistic regression model\n",
    "    if classifier_name == \"logit\":\n",
    "        params = {\n",
    "            \"penalty\": trial.suggest_categorical(\"penalty\", [\"l1\",\"l2\"]),\n",
    "            \"C\": trial.suggest_float(\"C\", 0.001, 10),\n",
    "            \"solver\": \"saga\",\n",
    "            \"max_iter\": 1000,\n",
    "            \"random_state\": RANDOM_SEED\n",
    "        }\n",
    "        \n",
    "        # Data standardization may improve accuracy of a logistic regression model\n",
    "        model = make_pipeline(StandardScaler(), LogisticRegression(**params))\n",
    "        \n",
    "    # When the model is a random forest model\n",
    "    elif classifier_name == \"rf\":\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "            \"criterion\": trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"]),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 1, 10),\n",
    "            \"min_samples_split\": trial.suggest_float(\"min_samples_split\", 0.1, 1),\n",
    "            \"random_state\": RANDOM_SEED\n",
    "        }\n",
    "    \n",
    "        model = RandomForestClassifier(**params)\n",
    "\n",
    "    model.fit(train_x, train_y)\n",
    "    predictions = model.predict_proba(test_x)[:, 1]\n",
    "    score = roc_auc_score(test_y, predictions)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = \"multimodel-breast-tumour\"\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED),\n",
    "    study_name=study_name\n",
    ")\n",
    "study.optimize(objective, n_trials=10)\n",
    "print(f\"The best roc_auc_score: {study.best_value}\")\n",
    "print(f\"The best hyperparameter combination: {study.best_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Ensemble averaging \n",
    "In some use cases, it may be useful to combine the outputs of multiple models to make the final prediction (ensemble models). One approach to make the final decision is to assign a weight to each model based on its performance, and these weights are used to determine the contribution of each model in the final prediction. For example, suppose there are $n$ models, whose predictions are denoted by $P_1$, $P_2$, ..., $P_n$, the models are assigned a weight of $w_1$, $w_2$, ..., $w_n$, respectively, the final prediction can then be calculated as:\n",
    "$$ final\\_prediction = {\\sum_{i=1}^{n}w_iP_i \\over \\sum_{i=1}^{n}w_i} $$\n",
    "\n",
    "In regression use cases, the *final_prediction* is simply the final result. In classification, the *final_prediction* can be the probability estimates, i.e., the likelihood of the input belonging to a class. \n",
    "\n",
    "Optuna can be used to find a good weight combination for combining the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, train three separate models and evaluate them. For simplicity, we just use default configurations for all hyperparameters (except for `random_state` which is set as a fixed value for reproducibility). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "rf_model = RandomForestClassifier(random_state=RANDOM_SEED)\n",
    "\n",
    "# logistic regression\n",
    "logit_model = make_pipeline(StandardScaler(), LogisticRegression(random_state=RANDOM_SEED)) \n",
    "\n",
    "# support vector classification\n",
    "svc_model = make_pipeline(StandardScaler(), SVC(random_state=RANDOM_SEED, probability=True))\n",
    "\n",
    "model_names = [\"rf_model\", \"logit_model\", \"svc_model\"]\n",
    "\n",
    "for name in model_names:\n",
    "    model = eval(name) # The eval() function evaluates a specified expression, if the expression is a legal Python statement, it will be executed.\n",
    "    model.fit(train_x, train_y)\n",
    "    predictions = model.predict_proba(test_x)[:, 1]\n",
    "    score = roc_auc_score(test_y, predictions)\n",
    "    print(f\"{name}: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best roc_auc_score is 0.9977 from the logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use Optuna to find a good weight combination to combine the outputs of these three models and see if this can improve the final roc_auc_score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a dictionary where each key is a model name, \n",
    "# and the corresponding value is an array of the predicted probabilities (for the class with the greater label) made by the model.\n",
    "all_predictions = {name: (eval(name)).predict_proba(test_x)[:, 1] for name in model_names}\n",
    "\n",
    "def objective(trial):\n",
    "    # This is a dictionary where each key is a model name, \n",
    "    # and the corresponding value is the weight assigned to the model.\n",
    "    weights = {name: trial.suggest_int(name, 1, 100) for name in model_names}\n",
    "\n",
    "    # [weights[name] * all_predictions[name] for name in model_names] forms a list of arrays, \n",
    "    # where each array is the weighted predictions of a model. This is done for each model specified in model_names.\n",
    "    # Then we use np.sum(..., axis=0) to sum up weighted predictions across all models for each data point. This produces an array.\n",
    "    # Finally, we divide the array by the sum of weights.\n",
    "    probs = np.sum([weights[name] * all_predictions[name] for name in model_names], axis=0)/sum(weights.values())\n",
    "   \n",
    "    score = roc_auc_score(test_y, probs)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = \"ensemble-breast-tumour\"\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED),\n",
    "    study_name=study_name\n",
    ")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(f\"The best roc_auc_score: {study.best_value}\")\n",
    "print(f\"The best weight combination: {study.best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best roc_auc_score after combining the predictions of the three models is 0.9979, which is slightly better than the roc_auc_score of the best singe model (0.9977)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelization in Python\n",
    "One way to speed up hyperparameter optimization is multi-processing: utilizing multiple processes to evaluate different sets of hyperparameters simultaneously. One way to implement multi-processing in Python is to use the joblib package, a Python library used for efficient parallel computing (and data serialization.) \n",
    "\n",
    "Below, you will see an example of using the joblib package to implement multi-processing. The example shows the basic use of the joblib's parallel computing functionalities but doesn't touch Optuna. You will use the joblib package to parallelize an Optuna study in one of this week's assignments. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going to the use of joblib, let's define a simple function that calculates the square root of a given number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, math\n",
    "def my_square_root(number):\n",
    "    # Wait for 1 second before starting the calculation\n",
    "    time.sleep(1)\n",
    "    return math.sqrt(number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we run the function for 10 times sequentially, without parallelization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_count = 10\n",
    "start = time.time()\n",
    "for i in range(iteration_count):\n",
    "    my_square_root(i)\n",
    "end = time.time()\n",
    "print(f\"Consumed time: {end - start}s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the `my_square_root` function needs to wait for 1s before starting the calculation, it takes around 10s to run this function 10 times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's run the function 10 times again, but we parallelize these 10 runs using 2 processes at this time. The [joblib.Parallel](https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html) class is needed to implement multi-processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "start = time.time()\n",
    "Parallel(n_jobs=2)(delayed(my_square_root)(i) for i in range(iteration_count))\n",
    "end = time.time()\n",
    "print(f\"Consumed time: {end - start}s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the code `Parallel(n_jobs=2)(delayed(my_square_root)(i) for i in range(iteration_count))` into smaller parts.\n",
    "\n",
    "- `Parallel(n_jobs=2)`: The `Parallel` class is used to set up a parallel processing context with two processes (n_jobs=2).\n",
    "\n",
    "- `delayed(...)`: The `delayed` function takes a function (`my_square_root` in the example) and generates a \"delayed\" task of it. In other words, this \"delayed\" task of the function will be executed later in parallel instead of being executed immediately. \n",
    "\n",
    "- `delayed(my_square_root)(i) for i in range(iteration_count)`: The `delayed` function generates `iteration_count` (which is 10 in the example) tasks to be executed in parallel, each calling the `my_square_root` function with arguments from 0 to `iteration_count-1`. \n",
    "\n",
    "- `Parallel(...)(...)`: The Parallel object returned from `Parallel(n_jobs=2)` is invoked to perform the delayed tasks generated by the `delayed` function. The multi-processing computation is started.\n",
    "\n",
    "Since the 10 runs of the `my_square_root` function are parallelized in two processes, the consumed time is around halved compared to the non-parallelized version. (The time is always a bit more than exactly half of the initial time because it takes time for the processes to communicate and coordinate with each other.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
