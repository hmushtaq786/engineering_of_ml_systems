{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepchecks tutorial\n",
    "[Deepchecks](https://docs.deepchecks.com/stable/getting-started/welcome.html) is a Python library for validating ML data and models. In the example below, you'll see how to use Deepchecks to evaluate a trained model before deploying the model to production (offline model evaluation). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deepchecks example\n",
    "Similar to the MLflow tutorial, we'll also train a ElasticNet model for red wine quality prediction in this example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "from deepchecks.tabular.checks import TrainTestPerformance, DatasetsSizeComparison\n",
    "from deepchecks.tabular import Dataset, Suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a red wine model\n",
    "\n",
    "csv_url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "    \n",
    "\n",
    "data = pd.read_csv(csv_url, sep=\";\")\n",
    "\n",
    "# Split the data into training and test sets. (0.6, 0.4) split.\n",
    "train, test = train_test_split(data, test_size=0.4, random_state=42)\n",
    "# The predicted column is \"quality\" which is a scalar from [3, 9]\n",
    "train_x = train.drop([\"quality\"], axis=1)\n",
    "test_x = test.drop([\"quality\"], axis=1)\n",
    "train_y = train[[\"quality\"]]\n",
    "test_y = test[[\"quality\"]]\n",
    "\n",
    "# Just use hard-coded parameters\n",
    "alpha = 0.5\n",
    "l1_ratio = 0.5\n",
    "\n",
    "lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n",
    "lr.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use Deepchecks to evaluate how the trained model performs on both training and testing dataset. Specifically, we'll evaluate the model's mean absolute error (MAE) and root mean square error (RMSE) on the training and testing dataset. \n",
    "\n",
    "As per the Deepchecks documentation, Deepchecks follows the convention that greater metric value represent better performance. Therefore, it is recommended to only use metrics that follow this convention, for example, negative MAE instead of MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the training and testing dataset into a format that Deepchecks can handle\n",
    "train_dataset = Dataset(df=train_x, label=train_y, cat_features=[])\n",
    "test_dataset = Dataset(df=test_x, label=test_y, cat_features=[])\n",
    "\n",
    "# Define a test, the \"scorers\" parameter specifies the metrics (negative MAE and negative RMSE) to be used in the test\n",
    "check = TrainTestPerformance(scorers=[\"neg_mae\", \"neg_rmse\"])\n",
    "result = check.run(train_dataset, test_dataset, lr)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "<img src=\"../images/deepchecks-single-test.png\" width=800>\n",
    "\n",
    "Suppose we have a baseline model that produces MAE and RMSE of 0.9 on the same testing dataset, we may draw two conclusions from the evaluation result:\n",
    "1) This model seems to be able to make better prediction than the baseline model.\n",
    "2) The MAE and RMSE don't seem to drop significantly on testing dataset, which can be seen as a good signal: the model isn't overfitting the training dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides a single test, Deepchecks provides a convenient way of grouping multiple tests into a collection, namely a Suite. In the example below, we specify a Suite containing two tests: 1) the familiar MAE and RMSE tests on both training and testing dataset, 2) comparing the size of testing dataset against the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a suite\n",
    "suite = Suite(\"Example test suite\", \n",
    "    TrainTestPerformance(scorers=[\"neg_mae\", \"neg_rmse\"]),\n",
    "    DatasetsSizeComparison()\n",
    "    # More tests can be added here\n",
    ")\n",
    "result = suite.run(model=lr, train_dataset=train_dataset, test_dataset=test_dataset)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "<img src=\"../images/deepchecks-suite.png\" width=800>\n",
    "\n",
    "If you click the \"Other\" part in the output result above, you can first see the same result of MAE and RMSE you got when running the second-to-last code cell. Moreover, you can also see the results of dataset size comparison between the training and testing dataset: \n",
    "\n",
    "<img src=\"../images/deepchecks-suite-dataset-size.png\" width=500>\n",
    "\n",
    "The \"Didn't pass\" and \"Passed\" parts are empty because we haven't specify [conditions](https://docs.deepchecks.com/stable/general/usage/customizations/auto_examples/plot_configure_check_conditions.html) that determine whether a test succeeds or not. You'll explore more about the test conditions in this week's assignments. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops_eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
